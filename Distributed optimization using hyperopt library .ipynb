{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel execution of Hyperopt optimization using Spark\n",
    "\n",
    "With the new class SparkTrials, you can tell Hyperopt to distribute a tuning job across an Apache Spark cluster. Initially developed within Databricks, this API has now been contributed to Hyperopt.\n",
    "\n",
    "Hyperparameter tuning and model selection often involve training hundreds or thousands of models.  SparkTrials runs batches of these training tasks in parallel, one on each Spark executor, allowing massive scale-out for tuning. To use SparkTrials with Hyperopt, simply pass the SparkTrials object to Hyperopt’s fmin() function.\n",
    "\n",
    "Under the hood, fmin() will generate new hyperparameter settings to test and pass them to SparkTrials, which runs these tasks asynchronously on a cluster as follows:\n",
    "\n",
    "- Hyperopt’s primary logic runs on the Spark driver, computing new hyperparameter settings.\n",
    "- When a worker is ready for a new task, Hyperopt kicks off a single-task Spark job for that hyperparameter setting.\n",
    "- Within that task, which runs on one Spark executor, user code will be executed to train and evaluate a new ML model.\n",
    "- When done, the Spark task will return the results, including the loss, to the driver.\n",
    "\n",
    "These new results are used by Hyperopt to compute better hyperparameter settings for future tasks.\n",
    "\n",
    "Since SparkTrials fits and evaluates each model on one Spark worker, it is limited to tuning single-machine ML models and workflows, such as scikit-learn or single-machine TensorFlow. For distributed ML algorithms such as Apache Spark MLlib or Horovod, you can use Hyperopt’s default Trials class.\n",
    "\n",
    "SparkTrials may be configured via 3 arguments, all of which are optional:\n",
    "\n",
    "1. parallelism The maximum number of trials to evaluate concurrently. Greater parallelism allows scale-out testing of more hyperparameter settings. Defaults to the number of Spark executors.\n",
    "\n",
    "    - Trade-offs: The parallelism parameter can be set in conjunction with the max_evals parameter in fmin(). Hyperopt will test max_evals total settings for your hyperparameters, in batches of size parallelism. If parallelism = max_evals, then Hyperopt will do Random Search: it will select all hyperparameter settings to test independently and then evaluate them in parallel. If parallelism = 1, then Hyperopt can make full use of adaptive algorithms like Tree of Parzen Estimators (TPE) which iteratively explore the hyperparameter space: each new hyperparameter setting tested will be chosen based on previous results. Setting parallelism in between 1 and max_evals allows you to trade off scalability (getting results faster) and adaptiveness (sometimes getting better models).\n",
    "    - Limits: There is currently a hard cap on parallelism of 128. SparkTrials will also check the cluster’s configuration to see how many concurrent tasks Spark will allow; if parallelism exceeds this maximum, SparkTrials will reduce parallelism to this maximum.\n",
    "    \n",
    "\n",
    "2. timeout Maximum time in seconds which fmin() is allowed to take, defaulting to None. Timeout provides a budgeting mechanism, allowing a cap on how long tuning can take. When the timeout is hit, runs are terminated if possible, and fmin() exits, returning the current set of results.\n",
    "\n",
    "3. spark_session SparkSession instance for SparkTrials to use. If this is not given, SparkTrials will look for an existing SparkSession.\n",
    "\n",
    "\n",
    "https://github.com/hyperopt/hyperopt/blob/master/hyperopt/spark.py\n",
    "\n",
    "http://hyperopt.github.io/hyperopt/scaleout/spark/\n",
    "http://hyperopt.github.io/hyperopt/scaleout/spark/#scaling-out-search-with-apache-spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### establishing a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Hyperopt').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using hyperopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin,tpe,hp, STATUS_OK, Trials\n",
    "from hyperopt import SparkTrials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SparkTrials in module hyperopt.spark:\n",
      "\n",
      "class SparkTrials(hyperopt.base.Trials)\n",
      " |  SparkTrials(parallelism=None, timeout=None, spark_session=None)\n",
      " |  \n",
      " |  Implementation of hyperopt.Trials supporting\n",
      " |  distributed execution using Apache Spark clusters.\n",
      " |  This requires fmin to be run on a Spark cluster.\n",
      " |  \n",
      " |  Plugging SparkTrials into hyperopt.fmin() allows hyperopt\n",
      " |  to send model training and evaluation tasks to Spark workers,\n",
      " |  parallelizing hyperparameter search.\n",
      " |  Each trial (set of hyperparameter values) is handled within\n",
      " |  a single Spark task; i.e., each model will be fit and evaluated\n",
      " |  on a single worker machine.  Trials are run asynchronously.\n",
      " |  \n",
      " |  See hyperopt.Trials docs for general information about Trials.\n",
      " |  \n",
      " |  The fields we store in our trial docs match the base Trials class.  The fields include:\n",
      " |   - 'tid': trial ID\n",
      " |   - 'state': JOB_STATE_DONE, JOB_STATE_ERROR, etc.\n",
      " |   - 'result': evaluation result for completed trial run\n",
      " |   - 'refresh_time': timestamp for last status update\n",
      " |   - 'misc': includes:\n",
      " |      - 'error': (error type, error message)\n",
      " |   - 'book_time': timestamp for trial run start\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SparkTrials\n",
      " |      hyperopt.base.Trials\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, parallelism=None, timeout=None, spark_session=None)\n",
      " |      :param parallelism: Maximum number of parallel trials to run,\n",
      " |                          i.e., maximum number of concurrent Spark tasks.\n",
      " |                          If set to None or and invalid value, this will be set to the number of\n",
      " |                          executors in your Spark cluster.\n",
      " |                          Hard cap at `MAX_CONCURRENT_JOBS_ALLOWED`.\n",
      " |                          Default: None (= number of Spark executors).\n",
      " |      :param timeout: Maximum time (in seconds) which fmin is allowed to take.\n",
      " |                      If this timeout is hit, then fmin will cancel running and proposed trials.\n",
      " |                      It will retain all completed trial runs and return the best result found\n",
      " |                      so far.\n",
      " |      :param spark_session: A SparkSession object. If None is passed, SparkTrials will attempt\n",
      " |                            to use an existing SparkSession or create a new one. SparkSession is\n",
      " |                            the entry point for various facilities provided by Spark. For more\n",
      " |                            information, visit the documentation for PySpark.\n",
      " |  \n",
      " |  count_cancelled_trials(self)\n",
      " |      Returns the current number of cancelled trial runs.\n",
      " |      This covers trials which are cancelled from exceeding the timeout.\n",
      " |  \n",
      " |  count_failed_trials(self)\n",
      " |      Returns the current number of trial runs which failed\n",
      " |  \n",
      " |  count_successful_trials(self)\n",
      " |      Returns the current number of trials which ran successfully\n",
      " |  \n",
      " |  count_total_trials(self)\n",
      " |      Returns the current number of all successful, failed, and cancelled trial runs\n",
      " |  \n",
      " |  delete_all(self)\n",
      " |      Reset the Trials to init state\n",
      " |  \n",
      " |  fmin(self, fn, space, algo, max_evals, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\n",
      " |      This should not be called directly but is called via :func:`hyperopt.fmin`\n",
      " |      Refer to :func:`hyperopt.fmin` for docs on each argument\n",
      " |  \n",
      " |  trial_attachments(self, trial)\n",
      " |      Support syntax for load:  self.trial_attachments(doc)[name]\n",
      " |      # -- does this work syntactically?\n",
      " |      #    (In any event a 2-stage store will work)\n",
      " |      Support syntax for store: self.trial_attachments(doc)[name] = value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  MAX_CONCURRENT_JOBS_ALLOWED = 128\n",
      " |  \n",
      " |  asynchronous = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from hyperopt.base.Trials:\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  aname(self, trial, name)\n",
      " |  \n",
      " |  assert_valid_trial(self, trial)\n",
      " |  \n",
      " |  average_best_error(self, bandit=None)\n",
      " |      Return the average best error of the experiment\n",
      " |      \n",
      " |      Average best error is defined as the average of bandit.true_loss,\n",
      " |      weighted by the probability that the corresponding bandit.loss is best.\n",
      " |      \n",
      " |      For domains with loss measurement variance of 0, this function simply\n",
      " |      returns the true_loss corresponding to the result with the lowest loss.\n",
      " |  \n",
      " |  count_by_state_synced(self, arg, trials=None)\n",
      " |      Return trial counts by looking at self._trials\n",
      " |  \n",
      " |  count_by_state_unsynced(self, arg)\n",
      " |      Return trial counts that count_by_state_synced would return if we\n",
      " |      called refresh() first.\n",
      " |  \n",
      " |  insert_trial_doc(self, doc)\n",
      " |      insert trial after error checking\n",
      " |      \n",
      " |      Does not refresh. Call self.refresh() for the trial to appear in\n",
      " |      self.specs, self.results, etc.\n",
      " |  \n",
      " |  insert_trial_docs(self, docs)\n",
      " |      trials - something like is returned by self.new_trial_docs()\n",
      " |  \n",
      " |  losses(self, bandit=None)\n",
      " |  \n",
      " |  new_trial_docs(self, tids, specs, results, miscs)\n",
      " |  \n",
      " |  new_trial_ids(self, N)\n",
      " |  \n",
      " |  refresh(self)\n",
      " |  \n",
      " |  source_trial_docs(self, tids, specs, results, miscs, sources)\n",
      " |  \n",
      " |  statuses(self, bandit=None)\n",
      " |  \n",
      " |  view(self, exp_key=None, refresh=True)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from hyperopt.base.Trials:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  argmin\n",
      " |  \n",
      " |  best_trial\n",
      " |      Trial with lowest non-NaN loss and status=STATUS_OK.\n",
      " |      If no such trial exists, returns None.\n",
      " |  \n",
      " |  idxs\n",
      " |  \n",
      " |  idxs_vals\n",
      " |  \n",
      " |  miscs\n",
      " |  \n",
      " |  results\n",
      " |  \n",
      " |  specs\n",
      " |  \n",
      " |  tids\n",
      " |  \n",
      " |  trials\n",
      " |  \n",
      " |  vals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SparkTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the objective function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    return {\n",
    "        'loss': x ** 2,\n",
    "        'status': STATUS_OK,\n",
    "        # -- store other results like this\n",
    "        'eval_time': time.time(),\n",
    "        'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
    "        # -- attachments are handled differently\n",
    "        'attachments':\n",
    "            {'time_module': pickle.dumps(time.time)}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_trials=SparkTrials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 100/100 [01:40<00:00,  1.01s/it, best loss: 0.000557585765298602]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Trials: 100: 100 succeeded, 0 failed, 0 cancelled.\n"
     ]
    }
   ],
   "source": [
    "best=fmin(objective,\n",
    "         space=hp.uniform('x',-10,10),\n",
    "         algo=tpe.suggest,\n",
    "         max_evals=100,\n",
    "         trials=spark_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [00:00<00:00, 761.52it/s, best loss: 0.005200315930984425]\n"
     ]
    }
   ],
   "source": [
    "#not using spark\n",
    "best=fmin(objective,\n",
    "         space=hp.uniform('x',-10,10),\n",
    "         algo=tpe.suggest,\n",
    "         max_evals=100,\n",
    "         trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 67,\n",
       " 'spec': None,\n",
       " 'result': {'loss': 0.005200315930984425,\n",
       "  'status': 'ok',\n",
       "  'eval_time': 1579012188.4286406,\n",
       "  'other_stuff': {'type': None, 'value': [0, 1, 2]}},\n",
       " 'misc': {'tid': 67,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'x': [67]},\n",
       "  'vals': {'x': [-0.07211321606324617]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2020, 1, 14, 14, 29, 48, 428000),\n",
       " 'refresh_time': datetime.datetime(2020, 1, 14, 14, 29, 48, 428000)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 63,\n",
       " 'spec': None,\n",
       " 'result': {'loss': 0.000557585765298602,\n",
       "  'status': 'ok',\n",
       "  'eval_time': 1579028504.032471,\n",
       "  'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "  'attachments': {'time_module': b'\\x80\\x03ctime\\ntime\\nq\\x00.'}},\n",
       " 'misc': {'tid': 63,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'x': [63]},\n",
       "  'vals': {'x': [0.023613254017576697]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2020, 1, 14, 19, 1, 41, 562000),\n",
       " 'refresh_time': datetime.datetime(2020, 1, 14, 19, 1, 44, 94000)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum loss attained with TPE:    0.0006\n"
     ]
    }
   ],
   "source": [
    "print('Minimum loss attained with TPE:    {:.4f}'.format(spark_trials.best_trial['result']['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>iteration</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.229794</td>\n",
       "      <td>0</td>\n",
       "      <td>1.493250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.622647</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.533773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.918291</td>\n",
       "      <td>2</td>\n",
       "      <td>8.827134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.070993</td>\n",
       "      <td>3</td>\n",
       "      <td>8.606451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.109970</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.471835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  iteration         x\n",
       "0   2.229794          0  1.493250\n",
       "1  30.622647          1 -5.533773\n",
       "2  77.918291          2  8.827134\n",
       "3  74.070993          3  8.606451\n",
       "4   6.109970          4 -2.471835"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spark_results = pd.DataFrame({'loss': [x['loss'] for x in spark_trials.results], 'iteration': spark_trials.idxs_vals[0]['x'],\n",
    "                            'x': spark_trials.idxs_vals[1]['x']})\n",
    "                            \n",
    "spark_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
